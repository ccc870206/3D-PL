<!DOCTYPE html>
<!-- modified from url=https://fuenwang.ml/project/led2net/ -->
<!-- <html lang="en" class="gr__ee_nycu_edu"> -->
<html lang="en">

<head>
<meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="author" content="ginawu">

<title>3D-PL</title>

<!-- CSS includes -->
<link href="static/asset/bootstrap.min.css" rel="stylesheet">
<link href="static/asset/css" rel="stylesheet" type="text/css">
<link href="static/asset/mystyle.css" rel="stylesheet">
<link href="static/asset/fig_style.css" rel="stylesheet">
</head>

<body data-gr-c-s-loaded="true">

<!-- <div class="topnav" id="myTopnav">
  <a href="#header">Home</a>
  <a href="#abstract">Abstract</a>
  <a href="#demo">Demo</a>
  <a href="#paper">Paper</a>
  <a href="#code">Code</a>
  <a href="javascript:void(0);" class="icon" onclick="toggleTopNav()">&#9776;</a>
</div> -->


<div id="header" class="container-fluid">
    <div class="row">
        <h1>3D-PL: Domain Adaptive Depth Estimation <br>with 3D-aware Pseudo-Labeling</h1>

        <div class="authors">
            <a href="mailto:ytyen.cs09g@nctu.edu.tw" target="_blank">Yu-Ting Yen<sup>1,2</sup></a>,
            <a href="mailto:julialu67.cs08g@nctu.edu.tw" target="_blank">Chia-Ni Lu<sup>1</sup></a>,
            <a href="https://walonchiu.github.io/" target="_blank">Wei-Chen Chiu<sup>1</sup></a>,
            <a href="https://sites.google.com/site/yihsuantsai/" target="_blank">Yi-Hsuan Tsai<sup>2</sup></a>
            <!-- <center>(* denotes equal contribution)</center> -->

            <p style="text-align:center;">
                <sup>1</sup>National Chiao Tung University, Taiwan &nbsp
                <sup>2</sup>Phiar Technologies
            </p>
        </div>
    </div>
</div>

<div class="container" id="paper">
    <center>
		<div class="mx-auto">
			<ul class="nav">
				<li class="nav-item text-center" style="display: inline-block;">
					<a href="https://github.com/ccc870206/3D-PL" class="nav-link">
						<svg style="width:50px;height:50px;" viewBox="0 0 16 16">
							<path fill="currentColor" d="M14 4.5V14a2 2 0 0 1-2 2H4a2 2 0 0 1-2-2V2a2 2 0 0 1 2-2h5.5L14 4.5zm-3 0A1.5 1.5 0 0 1 9.5 3V1H4a1 1 0 0 0-1 1v12a1 1 0 0 0 1 1h8a1 1 0 0 0 1-1V4.5h-2z"/>
  							<path fill="currentColor" d="M4.5 12.5A.5.5 0 0 1 5 12h3a.5.5 0 0 1 0 1H5a.5.5 0 0 1-.5-.5zm0-2A.5.5 0 0 1 5 10h6a.5.5 0 0 1 0 1H5a.5.5 0 0 1-.5-.5zm1.639-3.708 1.33.886 1.854-1.855a.25.25 0 0 1 .289-.047l1.888.974V8.5a.5.5 0 0 1-.5.5H5a.5.5 0 0 1-.5-.5V8s1.54-1.274 1.639-1.208zM6.25 6a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5z"/>
						</svg><br>
						Paper
					</a>
				<li class="nav-item text-center" style="display: inline-block;">
					<a href="https://github.com/ccc870206/3D-PL" class="nav-link">
						<svg style="width:50px;height:50px" viewBox="0 0 16 16">
							<path fill="currentColor" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z">
						</svg><br>
						Code
					</a>
			</ul>
		</div>
    </center>
</div>

<div class="container" id="teaser">
    <div class="row">
        <p style="text-align:center;">
            <img src="static/fig/teaser.jpg" width=95%>
        </p>
    </div>
</div>


<div class="container" id="abstract">
    <h2>Abstract</h2>
    <p style="text-align: justify;">
        For monocular depth estimation, acquiring ground truths for real data is not easy, and thus domain adaptation methods are commonly adopted using the supervised synthetic data. However, this may still incur a large domain gap due to the lack of supervision from the real data. In this paper, we develop a domain adaptation framework via generating reliable pseudo ground truths of depth from real data to provide direct supervisions.
        Specifically, we propose two mechanisms for pseudo-labeling: 1) 2D-based pseudo-labels via measuring the consistency of depth predictions when images are with the same content but different styles; 2) 3D-aware pseudo-labels via a point cloud completion network that learns to complete the depth values in the 3D space, thus providing more structural information in a scene to refine and generate more reliable pseudo-labels.
        In experiments, we show that our pseudo-labeling methods improve depth estimation in various settings, including the usage of stereo pairs during training. Furthermore, the proposed method performs favorably against several state-of-the-art unsupervised domain adaptation approaches in real-world datasets.
    </p>
</div>


<div class="container" id="qua_single">
    <h2>Synthetic-to-Real Benchmark: KITTI (single-image setting)</h2>
    <div class="row">
        <div class="column_4">
            <figure>
                <figcaption style="padding: 0px 0 15px">
                    Input Images
                </figcaption>
                <img src="static/fig/qualitative_no_stereo/input.png" width=98% style="float:right;">
                
            </figure>
        </div>
        <div class="column_4">
            <figure>
                <figcaption style="padding: 0px 0 15px">
                    Ground Truth
                </figcaption>
                <img src="static/fig/qualitative_no_stereo/gt.png" width=98% style="float:right;">
            </figure>
        </div>
        <div class="column_4">
            <figure>
                <figcaption style="padding: 0px 0 15px">
                    <a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Chuanxia_Zheng_T2Net_Synthetic-to-Realistic_Translation_ECCV_2018_paper.pdf">T<sup>2</sup>net</a>
                </figcaption>
                <img src="static/fig/qualitative_no_stereo/t2net.png" width=98% style="float:right;">
            </figure>
        </div>
        <div class="column_4">
            <figure>
                <figcaption style="padding: 0px 0 15px">
                    3D-PL
                </figcaption>
                <img src="static/fig/qualitative_no_stereo/ours.png" width=98% style="float:right;">
            </figure>
        </div>
    </div>

    <div class="row">
        <div class="column_4">
            <figure>
                <img src="static/fig/qualitative_no_stereo_more/input.png" width=98% style="float:right;">
                
            </figure>
        </div>
        <div class="column_4">
            <figure>
                <img src="static/fig/qualitative_no_stereo_more/gt.png" width=98% style="float:right;">
            </figure>
        </div>
        <div class="column_4">
            <figure>
                <img src="static/fig/qualitative_no_stereo_more/t2net.png" width=98% style="float:right;">
            </figure>
        </div>
        <div class="column_4">
            <figure>
                <img src="static/fig/qualitative_no_stereo_more/ours.png" width=98% style="float:right;">
            </figure>
        </div>
    </div>    

</div>


<div class="container" id="qua_stereo">
    <h2>Synthetic-to-Real Benchmark: KITTI (stereo-pair setting)</h2>
    <div class="row">
        <div class="column_5">
            <figure>
                <figcaption style="padding: 0px 0 15px">
                    Input Images
                </figcaption>
                <img src="static/fig/qualitative_stereo/input.png" width=98% style="float:right;">
                
            </figure>
        </div>
        <div class="column_5">
            <figure>
                <figcaption style="padding: 0px 0 15px">
                    Ground Truth
                </figcaption>
                <img src="static/fig/qualitative_stereo/gt.png" width=98% style="float:right;">
            </figure>
        </div>
        <div class="column_5">
            <figure>
                <figcaption style="padding: 0px 0 15px">
                    <a href="https://www.bmvc2020-conference.com/assets/papers/0122.pdf">DESC + Stereo</a>
                </figcaption>
                <img src="static/fig/qualitative_stereo/desc.png" width=98% style="float:right;">
            </figure>
        </div>
        <div class="column_5">
            <figure>
                <figcaption style="padding: 0px 0 15px">
                    <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/PNVR_SharinGAN_Combining_Synthetic_and_Real_Data_for_Unsupervised_Geometry_Estimation_CVPR_2020_paper.pdf">SharinGAN (Stereo)</a>
                </figcaption>
                <img src="static/fig/qualitative_stereo/sharingan.png" width=98% style="float:right;">
            </figure>
        </div>
        <div class="column_5">
            <figure>
                <figcaption style="padding: 0px 0 15px">
                    3D-PL + Stereo
                </figcaption>
                <img src="static/fig/qualitative_stereo/ours.png" width=98% style="float:right;">
            </figure>
        </div>
    </div>

    <div class="row">
        <div class="column_5">
            <figure>
                <img src="static/fig/qualitative_stereo_more/input.png" width=98% style="float:right;">
                
            </figure>
        </div>
        <div class="column_5">
            <figure>
                <img src="static/fig/qualitative_stereo_more/gt.png" width=98% style="float:right;">
            </figure>
        </div>
        <div class="column_5">
            <figure>
                <img src="static/fig/qualitative_stereo_more/desc.png" width=98% style="float:right;">
            </figure>
        </div>
        <div class="column_5">
            <figure>
                <img src="static/fig/qualitative_stereo_more/sharingan.png" width=98% style="float:right;">
            </figure>
        </div>
        <div class="column_5">
            <figure>
                <img src="static/fig/qualitative_stereo_more/ours.png" width=98% style="float:right;">
            </figure>
        </div>
    </div>

</div>


<div class="container" id="qua_make3d">
    <h2>Generalization to Real-world Datasets: Make3D</h2>
    <div class="row">
        <div class="column_4">
            <figure>
                <figcaption style="padding: 0px 0 15px">
                    Input Images
                </figcaption>
                <img src="static/fig/qualitative_make3d/input.png" width=98% style="float:right;">
                
            </figure>
        </div>
        <div class="column_4">
            <figure>
                <figcaption style="padding: 0px 0 15px">
                    Ground Truth
                </figcaption>
                <img src="static/fig/qualitative_make3d/gt.png" width=98% style="float:right;">
            </figure>
        </div>
        <div class="column_4">
            <figure>
                <figcaption style="padding: 0px 0 15px">
                    <a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Chuanxia_Zheng_T2Net_Synthetic-to-Realistic_Translation_ECCV_2018_paper.pdf">T<sup>2</sup>net</a>
                </figcaption>
                <img src="static/fig/qualitative_make3d/t2net.png" width=98% style="float:right;">
            </figure>
        </div>
        <div class="column_4">
            <figure>
                <figcaption style="padding: 0px 0 15px">
                    3D-PL
                </figcaption>
                <img src="static/fig/qualitative_make3d/ours.png" width=98% style="float:right;">
            </figure>
        </div>
    </div>
</div>


<br>
<br>


<!-- Javascript includes -->
<script src="static/asset/jquery-1.8.3.min.js"></script>
<script src="static/asset/mystyle.js"></script>
<script src="static/asset/bootstrap.min.js"></script>
<script async="" src="static/asset/analytics.js"></script><script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
 (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
 m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
 })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-98479202-1', 'auto');
ga('send', 'pageview');
</script>

<div id="point-jawn" style="z-index: 2147483647;"></div></body></html>

