<!DOCTYPE html>
<!-- modified from url=https://fuenwang.ml/project/led2net/ -->
<!-- <html lang="en" class="gr__ee_nycu_edu"> -->
<html lang="en">

<head>
<meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="author" content="ginawu">

<title>Inverse Halftone Colorization</title>

<!-- CSS includes -->
<link href="static/asset/bootstrap.min.css" rel="stylesheet">
<link href="static/asset/css" rel="stylesheet" type="text/css">
<link href="static/asset/mystyle.css" rel="stylesheet">
<link href="static/asset/fig_style.css" rel="stylesheet">
</head>

<body data-gr-c-s-loaded="true">

<!-- <div class="topnav" id="myTopnav">
  <a href="#header">Home</a>
  <a href="#abstract">Abstract</a>
  <a href="#demo">Demo</a>
  <a href="#paper">Paper</a>
  <a href="#code">Code</a>
  <a href="javascript:void(0);" class="icon" onclick="toggleTopNav()">&#9776;</a>
</div> -->


<div id="header" class="container-fluid">
    <div class="row">
        <h1>3D-PL: Domain Adaptive Depth Estimation with 3D-aware Pseudo-Labeling</h1>

        <div class="authors">
            <a href="mailto:ytyen.cs09g@nctu.edu.tw" target="_blank">Yu-Ting Yen</a>,
            <a href="mailto:julialu67.cs08g@nctu.edu.tw" target="_blank">Chia-Ni Lu</a>,
            <a href="https://walonchiu.github.io/" target="_blank">Wei-Chen Chiu</a>
            <a href="https://sites.google.com/site/yihsuantsai/" target="_blank">Yi-Hsuan Tsai</a>
            <!-- <center>(* denotes equal contribution)</center> -->

            <p style="text-align:center;">
                National Chiao Tung University, Taiwan 
                <br>
                Phiar Technologies
            </p>
        </div>
    </div>
</div>

<div class="container" id="paper">
    <center>
		<div class="mx-auto">
			<ul class="nav">
				<li class="nav-item text-center" style="display: inline-block;">
					<a href="https://github.com/ccc870206/3D-PL" class="nav-link">
						<svg style="width:50px;height:50px;" viewBox="0 0 16 16">
							<path fill="currentColor" d="M14 4.5V14a2 2 0 0 1-2 2H4a2 2 0 0 1-2-2V2a2 2 0 0 1 2-2h5.5L14 4.5zm-3 0A1.5 1.5 0 0 1 9.5 3V1H4a1 1 0 0 0-1 1v12a1 1 0 0 0 1 1h8a1 1 0 0 0 1-1V4.5h-2z"/>
  							<path fill="currentColor" d="M4.5 12.5A.5.5 0 0 1 5 12h3a.5.5 0 0 1 0 1H5a.5.5 0 0 1-.5-.5zm0-2A.5.5 0 0 1 5 10h6a.5.5 0 0 1 0 1H5a.5.5 0 0 1-.5-.5zm1.639-3.708 1.33.886 1.854-1.855a.25.25 0 0 1 .289-.047l1.888.974V8.5a.5.5 0 0 1-.5.5H5a.5.5 0 0 1-.5-.5V8s1.54-1.274 1.639-1.208zM6.25 6a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5z"/>
						</svg><br>
						Paper
					</a>
				<li class="nav-item text-center" style="display: inline-block;">
					<a href="https://github.com/ccc870206/3D-PL" class="nav-link">
						<svg style="width:50px;height:50px" viewBox="0 0 16 16">
							<path fill="currentColor" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.012 8.012 0 0 0 16 8c0-4.42-3.58-8-8-8z">
						</svg><br>
						Code
					</a>
			</ul>
		</div>
    </center>
</div>

<!-- <div class="container" id="teaser">
    <div class="row">
        <p style="text-align:center;">
            <img src="static/fig/teaser.png" width=80%>
        </p>
    </div>
</div> -->

<div class="container" id="abstract">
    <h2>Abstract</h2>
    <p style="text-align: justify;">
        For monocular depth estimation, acquiring ground truths for real data is not easy, and thus domain adaptation methods are commonly adopted using the supervised synthetic data. However, this may still incur a large domain gap due to the lack of supervision from the real data. In this paper, we develop a domain adaptation framework via generating reliable pseudo ground truths of depth from real data to provide direct supervisions.
        Specifically, we propose two mechanisms for pseudo-labeling: 1) 2D-based pseudo-labels via measuring the consistency of depth predictions when images are with the same content but different styles; 2) 3D-aware pseudo-labels via a point cloud completion network that learns to complete the depth values in the 3D space, thus providing more structural information in a scene to refine and generate more reliable pseudo-labels.
        In experiments, we show that our pseudo-labeling methods improve depth estimation in various settings, including the usage of stereo pairs during training. Furthermore, the proposed method performs favorably against several state-of-the-art unsupervised domain adaptation approaches in real-world datasets.
    </p>
</div>

<!-- <div class="container" id="video">
    <h2>Presentation Video</h2>
	<center>
	<div class="embed-responsive embed-responsive-16by9">
        <iframe width="900" height="506" src="https://www.youtube.com/embed/1LeQmx5QEts" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
	</div>
	</center>
</div> -->

<!-- <div class="container" id="demo">
    <h2>Comparisons with Baseline Methods</h2>
    <div class"row">
        <figure>
            <img src="static/fig/comparisons/input.png" width=88% style="float:right;">
            <figcaption style="padding: 28px 0 32px">
                Input Images
            </figcaption>
        </figure>
    </div>
    <div class"row">
        <figure>
            <img src="static/fig/comparisons/CA.png" width=88% style="float:right;">
            <figcaption style="padding: 28px 0 32px">
                <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_Generative_Image_Inpainting_CVPR_2018_paper.pdf">CA</a>
            </figcaption>
        </figure>
    </div>
    <div class"row">
        <figure>
            <img src="static/fig/comparisons/PEN-Net.png" width=88% style="float:right;">
            <figcaption style="padding: 28px 0 32px">
                <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zeng_Learning_Pyramid-Context_Encoder_Network_for_High-Quality_Image_Inpainting_CVPR_2019_paper.pdf">PEN-Net</a>
            </figcaption>
        </figure>
    </div>
    <div class"row">
        <figure>
            <img src="static/fig/comparisons/StructureFlow.png" width=88% style="float:right;">
            <figcaption style="padding: 28px 0 32px">
                <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Ren_StructureFlow_Image_Inpainting_via_Structure-Aware_Appearance_Flow_ICCV_2019_paper.pdf">StructureFlow</a>
            </figcaption>
        </figure>
    </div>
    <div class"row">
        <figure>
            <img src="static/fig/comparisons/HiFill.png" width=88% style="float:right;">
            <figcaption style="padding: 28px 0 32px">
                <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Yi_Contextual_Residual_Aggregation_for_Ultra_High-Resolution_Image_Inpainting_CVPR_2020_paper.pdf">HiFill</a>
            </figcaption>
        </figure>
    </div>
    <div class"row">
        <figure>
            <img src="static/fig/comparisons/ProFill.png" width=88% style="float:right;">
            <figcaption style="padding: 28px 0 32px">
                <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123640001.pdf">ProFill</a>
            </figcaption>
        </figure>
    </div>
    <div class"row">
        <figure>
            <img src="static/fig/comparisons/SRN.png" width=88% style="float:right;">
            <figcaption style="padding: 28px 0 32px">
                <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Wide-Context_Semantic_Image_Extrapolation_CVPR_2019_paper.pdf">SRN</a>
            </figcaption>
        </figure>
    </div>
    <div class"row">
        <figure>
            <img src="static/fig/comparisons/Yang.png" width=88% style="float:right;">
            <figcaption style="padding: 28px 0 32px">
                <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Yang_Very_Long_Natural_Scenery_Image_Prediction_by_Outpainting_ICCV_2019_paper.pdf">Yang et. al.</a>
            </figcaption>
        </figure>
    </div>
    <div class"row">
        <figure>
            <img src="static/fig/comparisons/ours.png" width=88% style="float:right;">
            <figcaption style="padding: 28px 0 32px">
                Ours
            </figcaption>
        </figure>
    </div>
</div> -->

<!-- <div class="container" id="demo">
    <h2>Full Panoramic Results</h2>
    <div class"column">
        <p style="text-align:center;">
            Input Image 1<span style="margin-left: 43px;"></span>Input Image 2<span style="margin-left: 250px;"></span>Resultant Full Panoramic Image<span style="margin-left: 220px;"></span>
        </p>
        <figure>
            <center><img src="static/fig/panorama/panorama_1.png" width=90%></center>
        </figure>
        <figure>
            <center><img src="static/fig/panorama/panorama_2.png" width=90%></center>
        </figure>
        <figure>
            <center><img src="static/fig/panorama/panorama_3.png" width=90%></center>
        </figure>
        <figure>
            <center><img src="static/fig/panorama/panorama_4.png" width=90%></center>
        </figure>
        <figure>
            <center><img src="static/fig/panorama/panorama_5.png" width=90%></center>
        </figure>
        <figure>
            <center><img src="static/fig/panorama/panorama_6.png" width=90%></center>
        </figure>
        <figure>
            <center><img src="static/fig/panorama/panorama_7.png" width=90%></center>
        </figure>
        <figure>
            <center><img src="static/fig/panorama/panorama_8.png" width=90%></center>
        </figure>
    </div>
</div> -->

<!-- <div class="container" id="video">
    <h2>Supplementary Materials</h2>
	<div class="container" id="paper">
        <center>
            <div class="mx-auto">
                <ul class="nav">
                    <li class="nav-item text-center" style="display: inline-block;">
                            <a href="https://drive.google.com/file/d/1cw6W1Sfq8JOJt2-NONOGYxorjfCXzgeW/view?usp=sharing" class="nav-link">
                                <svg style="width:50px;height:50px;" viewBox="0 0 16 16">
                                    <path fill="currentColor" d="M14 4.5V14a2 2 0 0 1-2 2H4a2 2 0 0 1-2-2V2a2 2 0 0 1 2-2h5.5L14 4.5zm-3 0A1.5 1.5 0 0 1 9.5 3V1H4a1 1 0 0 0-1 1v12a1 1 0 0 0 1 1h8a1 1 0 0 0 1-1V4.5h-2z"/>
                                      <path fill="currentColor" d="M4.5 12.5A.5.5 0 0 1 5 12h3a.5.5 0 0 1 0 1H5a.5.5 0 0 1-.5-.5zm0-2A.5.5 0 0 1 5 10h6a.5.5 0 0 1 0 1H5a.5.5 0 0 1-.5-.5zm1.639-3.708 1.33.886 1.854-1.855a.25.25 0 0 1 .289-.047l1.888.974V8.5a.5.5 0 0 1-.5.5H5a.5.5 0 0 1-.5-.5V8s1.54-1.274 1.639-1.208zM6.25 6a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5z"/>
                                </svg><br>
                                Link
                            </a>
                </ul>
            </div>
        </center>
    </div>
</div> -->

<!-- <div class="container" id="video">
    <h2>Supplementary Video</h2>
	<center>
	<div class="embed-responsive embed-responsive-16by9">
    	<iframe width="900" height="506" src="https://youtube.com/embed/oni2XBXB9b4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
	</div>
	</center>
</div> -->

<!-- <div class="container" id="paper">
    <h3>Citation</h3>
    <div>
        <pre class="citation">
@InProceedings{yen2021inverse,
    title={Inverse Halftone Colorization: Making Halftone Prints Color Photos},
    author={Yen, Yu-Ting and Cheng, Chia-Chi and Chiu, Wei-Chen},
    booktitle={2021 IEEE International Conference on Image Processing (ICIP)},
    pages={1734--1738},
    year={2021},
    organization={IEEE}
}
		</pre>
    </div>
</div> -->

<br>

<!-- Javascript includes -->
<script src="static/asset/jquery-1.8.3.min.js"></script>
<script src="static/asset/mystyle.js"></script>
<script src="static/asset/bootstrap.min.js"></script>
<script async="" src="static/asset/analytics.js"></script><script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
 (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
 m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
 })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-98479202-1', 'auto');
ga('send', 'pageview');
</script>

<div id="point-jawn" style="z-index: 2147483647;"></div></body></html>